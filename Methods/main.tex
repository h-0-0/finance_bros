\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}


% PACKAGES 
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{array}
\usepackage[
backend=biber,
sorting=anyt,
defernumbers=true,
style=authoryear,
natbib
]{biblatex}
\addbibresource{references.bib}
\setlength\bibitemsep{\baselineskip}

% FORMATTING
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0em}
\usepackage[left = 2.5cm , right = 2.5cm, bottom = 2.7cm, top = 2.7cm]{geometry}
\usepackage{titling}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

%COMMANDS
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\argminun}{\underset{\mathbf{w}}{\argmin}}
\newcommand{\argmaxun}{\underset{\mathbf{w}}{\argmax}}
\newcommand{\bxi}{\mathbf{x}_i}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bphi}{\boldsymbol\phi}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\title{\vspace{-2cm}Title}
\author{Authors}
\date{\vspace{-5ex}}
\begin{document}
 
\maketitle

\begin{abstract}
In this document we will be using support vector machine (SVM) for financial time series forecasting of the value of the digital currency, Bitcoin. We will create several prediction models using SVM that will use different asset price data, which will be compared performance against each other to determine which assets are most associated to accurate predictions of Bitcoin.


\end{abstract}

\section{Introduction}
In this paper, we propose using support vector regression (SVR) to forecast the value of bitcoin using data from other cryptocurrencies and stocks. SVR is a powerful machine learning technique that has been successfully applied to a wide range of regression problems. By training an SVR model on a dataset of past cryptocurrency and stock values, we aim to build a model that can accurately predict the future value of bitcoin. In the following sections, we will describe the dataset and feature engineering steps taken to prepare the data for modeling, the SVR model training and evaluation process, and present the results and analysis of our forecasting approach.

\textbf{Citation} \citet{trafalisSVM}, need to include in text somewhere.


\section{Forecasting Methodology}
\textbf{Notation} (change formatting later)

We will denote our set of predictors by the matrix $\bX$, where the $i$th row is denoted by $\bx_i$, and we let $Y$ denote the vector of outcome observations. 

We define the SVM decision rule based on an affine function of the features which we denote as:

$$g(\bx;\bw) = \langle \bw',\bx \rangle + w_0$$

\subsection{Support Vector Regression (SVR)}
For Support Vector Regression (SVR) the goal is to find a function that estimates the target variable as a linear combination of the input variables, while minimizing the error between the predictions and the true values.

In standard linear regression the squared error loss is used, which can be motivated by making the probabilistic assumption that the outcome variables are normally distributed conditionally given the predictors.

A way to view support vector regression (SVR), is by introducing the $\boldsymbol{\epsilon}$-insensitive loss function.  
This is defined below:

\begin{align} \label{eq:binary linear classifier}
    E_{\epsilon}(y - g(\bx;\bw)) =
     \begin{cases}
       0, \quad &|g(\bx;\bw) - y| < \epsilon
       \\
       |g(\bx;\bw) - y| - \epsilon, &\text{otherwise}
     \end{cases}
\end{align}

In words, this loss function only punishes incorrect predictions when the discrepancy between the actual value and the predicted value is larger than some choice of threshold $\epsilon$.

The objective function for SVR can then be defined as regularised regression, using the $\boldsymbol{\epsilon}$-insensitive loss function.  
This is given by:

\begin{equation}
\sum_{i = 1}^{n}{E_{\epsilon}(y_i - g(\bx_i;\bw))} + \frac{1}{2}||\bw'||^2    
\end{equation}



Minimising the objective function given above, can then be rewritten as a constrained optimisation problem by introducing slack variables $\xi$, $\hat{\xi}$. Often an additional parameter $C$ is added.

This is given by:


\begin{align}
\argmin_{\xi,\hat{\xi},\bw'} \; C{\sum_{i = 1}^{n}}{(\xi_i + \hat{\xi_i})} + \frac{1}{2}||\bw'||^2 \nonumber\\
\text{s.t} \quad y_i - g(\bx_i;\bw') \leq \epsilon + \xi_i \label{eq:SVR formulation}\\
g(\bx_i;\bw') - y_i \leq \epsilon + \hat{\xi}_i \nonumber\\
\xi_i, \hat{\xi}_i \geq 0 \nonumber
\end{align}

The optimization problem for SVM regression can be formulated as follows:
minimize $1/2||w||^2 + C \sum \xi_i$
subject to $y_i - (w^T x_i + b) <= \epsilon + \xi_i$ , for all $i = 1, ..., n$ and $\xi_i >= 0$
where $w$ is the weight vector, $b$ is the bias, $C$ is a regularization parameter, $\epsilon$ is the margin width and $\xi_i$ are the error/slack variables. The vector $x_i$ and $y_i$ represent the input and output variables, respectively.

The goal is to find the weight vector, $w$ and bias $b$ that minimizes the objective function. To do this, a technique such as quadratic programming should be used. In addition, the constraints ensure that the difference between the predicted and true values of the target variable is at most $\epsilon + \xi_i$ for all instances. This allows for a certain degree of error, controlled by the parameter $\epsilon$, but penalizes larger errors through the term $C * \xi_i$.

It's important to mention that in this case, the prediction is a real value instead of a class label, so it's possible to have an error term, unlike in SVM classification. The cost function is a trade-off between a margin of tolerance to errors and a regularization term that avoids over-fitting. The parameter $C$ determines this trade-off. If $C$ is large, it puts more weight on the errors and tries to minimize them, while if $C$ is small it gives more importance to the regularization term which tries to keep the model simple.

\subsubsection{Dual Formulation}

In Support Vector Machines for classification the kernel trick can be implemented when using its dual form. The analogous reformulation can be carried out with SVR, so that the kernel trick can be used.

In order to fit non-linear decision boundaries we can employ whats called the kernel trick. One approach to fitting a non-linear decision boundary would be to try change the model we are fitting, ie. try and fit a non-linear model. An alternative approach and the one we will use here is to change the feature space of the data such that is is linearly separable. For example in 2D space we may have one class in the centre and another class surrounding it, the decision boundary here is a circle. With the kernel trick what we can do is add a third dimension where we might find the class in the middle takes smaller values and the class surrounding larger values, now there exists a linear decision boundary. The perk of using kernel functions here is we don't need to change our formulation of the SVM in order to fit a non-linear decision boundary. We introduce the kernel function by replacing the dot product of --- with a kernel function, k, which still computes the inner product but can do so in a much higher dimensional feature space. We can rewrite our dual representation of $\epsilon$-regression with a kernel function as follows:
\begin{align}
    &{} \min_{\alpha, \alpha^{*}} \quad \frac{1}{2} (\alpha - \alpha^{*})^{T} \mathbf{Q} (\alpha - \alpha^{*}) + \epsilon \sum_{i=1}^{l} (\alpha_{i} + \alpha_{i}^{*}) + \sum_{i=1}^{l} y_{i} (\alpha_{i} - \alpha_{i}^{*}) \\
    & \text{s.t.} \quad 0 \leq \alpha_{i}, \alpha_{i}^{*} \leq C, i=1,...,l, \sum_{i=1}^{l} (\alpha_{i} - \alpha_{i}^{*}) = 0
\end{align}
where $Q_{ij} = y_{i}  y_{j}  \cdot k(x_{i}, x_{j})$
%TODO: change equations to match previous terminology (in chapter 2.1)
%TODO: replace --- in paragraph above




\newpage


\newpage
\input{Methods/appendix.tex}

\section{Bibliography}
\printbibliography

\end{document}



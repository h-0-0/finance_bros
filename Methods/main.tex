\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}


% PACKAGES 
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{array}
\usepackage[
backend=biber,
sorting=anyt,
defernumbers=true,
style=authoryear,
natbib
]{biblatex}
\addbibresource{references.bib}
\setlength\bibitemsep{\baselineskip}

% FORMATTING
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0em}
\usepackage[left = 2.5cm , right = 2.5cm, bottom = 2.7cm, top = 2.7cm]{geometry}
\usepackage{titling}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

%COMMANDS
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\argminun}{\underset{\mathbf{w}}{\argmin}}
\newcommand{\argmaxun}{\underset{\mathbf{w}}{\argmax}}
\newcommand{\bxi}{\mathbf{x}_i}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bphi}{\boldsymbol\phi}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\title{\vspace{-2cm}Title}
\author{Authors}
\date{\vspace{-5ex}}
\begin{document}
 
\maketitle

\begin{abstract}
In this document we will be using support vector machine (SVM) for financial time series forecasting of the value of the digital currency, Bitcoin. We will create several prediction models using SVM that will use different asset price data, which will be compared performance against each other to determine which assets are most associated to accurate predictions of Bitcoin.  

\end{abstract}

\section{Introduction}
In this paper, we propose using support vector regression (SVR) to forecast the value of bitcoin using data from other cryptocurrencies and stocks. SVR is a powerful machine learning technique that has been successfully applied to a wide range of regression problems. By training an SVR model on a dataset of past cryptocurrency and stock values, we aim to build a model that can accurately predict the future value of bitcoin. In the following sections, we will describe the dataset and feature engineering steps taken to prepare the data for modeling, the SVR model training and evaluation process, and present the results and analysis of our forecasting approach.

\section{Forecasting Methodology}
\textbf{Notation} (change formatting later)

We will denote our set of predictors by the matrix $\bX$, where the $i$th row is denoted by $\bx_i$, and we let $Y$ denote the vector of outcome observations. 




\subsection{Support Vector Regression (SVR)}
For Support Vector Regression (SVR) the goal is to find a function that estimates the target variable as a linear combination of the input variables, while minimizing the error between the predictions and the true values.

In standard linear regression we use the squared error loss, which can be motivated by making the probabilistic assumption that the outcome variables are normally distributed conditionally given the predictors.

A way to view support vector regression (SVR) is by introducing the $\boldsymbol{\epsilon}$-insensistive loss function.  
This is defined below:

\begin{align} \label{eq:binary linear classifier}
    E_{\epsilon}(y - g(\bx;\bw)) =
     \begin{cases}
       0, \quad &|g(\bx;\bw) - y| < \epsilon
       \\
       |g(\bx;\bw) - y| - \epsilon, &\text{otherwise}
     \end{cases}
\end{align}

In words, this loss function only punishes incorrect predictions when the discrepancy between the actual value and the predicted value is larger than some choice of threshold $\epsilon$.

The objective function for SVR can be defined as regularised regression, using the $\boldsymbol{\epsilon}$-insensistive loss function.  
This is given by:

$$\sum_{i = 1}^{n}{E_{\epsilon}(y_i - g(\bx_i;\bw))} + \lambda||\bw'||^2$$

The optimization problem for SVM regression can be formulated as follows:
minimize $1/2||w||^2 + C \sum \xi_i$
subject to $y_i - (w^T x_i + b) <= \epsilon + \xi_i$ , for all $i = 1, ..., n$ and $\xi_i >= 0$
where $w$ is the weight vector, $b$ is the bias, $C$ is a regularization parameter, $\epsilon$ is the margin width and $\xi_i$ are the error/slack variables. The vector $x_i$ and $y_i$ represent the input and output variables, respectively.

The goal is to find the weight vector, $w$ and bias $b$ that minimizes the objective function. To do this, a technique such as quadratic programming should be used. In addition, the constraints ensure that the difference between the predicted and true values of the target variable is at most $\epsilon + \xi_i$ for all instances. This allows for a certain degree of error, controlled by the parameter $\epsilon$, but penalizes larger errors through the term $C * \xi_i$.

It's important to mention that in this case, the prediction is a real value instead of a class label, so it's possible to have an error term, unlike in SVM classification. The cost function is a trade-off between a margin of tolerance to errors and a regularization term that avoids over-fitting. The parameter $C$ determines this trade-off. If $C$ is large, it puts more weight on the errors and tries to minimize them, while if $C$ is small it gives more importance to the regularization term which tries to keep the model simple.

\citet{trafalisSVM}

\newpage

\citet{trafalisSVM}

\newpage
\input{Methods/appendix.tex}

\section{Bibliography}
\printbibliography

\end{document}


